{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sam_sub = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_tweet(text):\n",
    "\n",
    "  # take off html tags\n",
    "  text = BeautifulSoup(text).get_text()\n",
    "  \n",
    "  # fetch alphabetic characters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "  # convert text to lower case\n",
    "  text = text.lower()\n",
    "\n",
    "  # split text into tokens to remove whitespaces\n",
    "  tokens = text.split()\n",
    "\n",
    "  return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"clean_tweet\"] = train['tweet'].apply(clean_tweet)\n",
    "test[\"clean_tweet\"] =  test['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train[\"clean_tweet\"],\n",
    "                                                                    train[\"label\"], \n",
    "                                                                    test_size=0.3,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numpy arrays\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "# Tokenize texts with a maximum sequence length parameter\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=60,return_tensors='tf')  # Adjust max_length as needed\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=60,return_tensors='tf')  # Adjust max_length as needed\n",
    "test_encodings = tokenizer(test[\"clean_tweet\"].tolist(), truncation=True, padding=True, max_length=60,return_tensors='tf')\n",
    "# train_labels = np.array(train_labels)\n",
    "# val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=60,return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5544, 60), dtype=int32, numpy=\n",
       "array([[  101,  2293,  2026, ...,     0,     0,     0],\n",
       "       [  101,  2026,  2611, ...,     0,     0,     0],\n",
       "       [  101,  2047, 26381, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  2009,  2215, ...,     0,     0,     0],\n",
       "       [  101,  6302, 23205, ...,     0,     0,     0],\n",
       "       [  101,  2074,  2288, ...,     0,     0,     0]])>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Comments -->> love my apple watch apple watch mm excited christmas came early instagood https instagram com p r gadcn n\n",
      "\n",
      "Input Ids -->>\n",
      " tf.Tensor(\n",
      "[  101  2293  2026  6207  3422  6207  3422  3461  7568  4234  2234  2220\n",
      " 16021 15900 17139 16770 16021 23091  4012  1052  1054 11721 16409  2078\n",
      "  1050   102     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(60,), dtype=int32)\n",
      "\n",
      "Decoded Ids -->>\n",
      " [CLS] love my apple watch apple watch mm excited christmas came early instagood https instagram com p r gadcn n [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "Attention Mask -->>\n",
      " tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(60,), dtype=int32)\n",
      "\n",
      "Labels -->> 0\n"
     ]
    }
   ],
   "source": [
    "#Have a look of encoding data\n",
    "k = 0\n",
    "print('Training Comments -->>',train_texts.tolist()[k])\n",
    "print('\\nInput Ids -->>\\n',train_encodings['input_ids'][k])\n",
    "print('\\nDecoded Ids -->>\\n',tokenizer.decode(train_encodings['input_ids'][k]))\n",
    "print('\\nAttention Mask -->>\\n',train_encodings['attention_mask'][k])\n",
    "print('\\nLabels -->>',train_labels.tolist()[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(train_encodings),\n",
    "#     train_labels\n",
    "# ))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     dict(val_encodings),\n",
    "#     val_labels\n",
    "# ))\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "174/174 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.8636WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "174/174 [==============================] - 1911s 11s/step - loss: 0.2930 - accuracy: 0.8636 - val_loss: 0.2162 - val_accuracy: 0.9137\n",
      "Epoch 2/2\n",
      "174/174 [==============================] - 2058s 12s/step - loss: 0.1793 - accuracy: 0.9293 - val_loss: 0.2241 - val_accuracy: 0.9066\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_encodings['input_ids'], train_encodings['token_type_ids'], train_encodings['attention_mask']],\n",
    "    train_labels,\n",
    "    validation_data=(\n",
    "      [val_encodings['input_ids'], val_encodings['token_type_ids'], val_encodings['attention_mask']],val_labels),\n",
    "    batch_size=32,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 251s 3s/step - loss: 0.2241 - accuracy: 0.9066\n",
      "Test loss: 0.2240566909313202, Test accuracy: 0.9065656661987305\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(\n",
    "    [val_encodings['input_ids'], val_encodings['token_type_ids'], val_encodings['attention_mask']],\n",
    "    val_labels\n",
    ")\n",
    "print(f'Test loss: {val_loss}, Test accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(\n",
    "    [test_encodings['input_ids'], test_encodings['token_type_ids'], test_encodings['attention_mask']])\n",
    " \n",
    "# pred is of type TFSequenceClassifierOutput\n",
    "logits = pred.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = tf.argmax(logits, axis=1)\n",
    "predictions_label = list(pred_labels.numpy())\n",
    "predictions_label.count(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_sub[\"label\"]=predictions_label\n",
    "sam_sub.to_csv(\"sam_sub-BERT.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_practice",
   "language": "python",
   "name": "nlp_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
